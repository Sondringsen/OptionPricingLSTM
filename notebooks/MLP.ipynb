{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RcTGoUnAo_sG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HwI8mUIUwzO6"
      },
      "outputs": [],
      "source": [
        "#Variables\n",
        "first_year = 2019\n",
        "last_year = 2021\n",
        "split_date =\"2021-01-01\"\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "features = [\"Underlying_last\", \"Strike\", \"Ttl\", \"Volatility\", \"R\"]\n",
        "num_features = len(features)\n",
        "num_outputs = 1\n",
        "seq_length = 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cd-EqCZ_1Bok"
      },
      "outputs": [],
      "source": [
        "def read_file(file):\n",
        "    \"\"\"Read a single file and return a dataframe\"\"\"\n",
        "    return pd.read_csv(file, skipinitialspace=True)\n",
        "\n",
        "def lag_features(df, features, seq_length):\n",
        "    \"\"\"Transforms a raw 2D dataframe of option data into 2D dataframe ofsequence data.\n",
        "    Last 2 indexes per sequence are bid and ask price. The len(features)*seq_length\n",
        "    features before are sequences of features\"\"\"\n",
        "    df = df.sort_values([\"Expire_date\", \"Strike\", \"Ttl\"], ascending = [True, True, False])\n",
        "\n",
        "    for step in range(seq_length)[::-1]:\n",
        "        for feature in features:\n",
        "            df[feature + \"-\" + str(step)] = df[feature].shift(step)\n",
        "    \n",
        "    df[\"Check_strike\"] = df[\"Strike\"] == df[\"Strike\"].shift(seq_length-1)\n",
        "    df[\"Check_expire\"] = df[\"Expire_date\"] == df[\"Expire_date\"].shift(seq_length-1)\n",
        "    df = df[(df[\"Check_strike\"] == True) & (df[\"Check_expire\"] == True)]\n",
        "    df = df.drop([\"Check_strike\", \"Check_expire\"], axis=1)\n",
        "    #df[[\"Bid_strike_last\", \"Ask_strike_last\"]] = df[[\"Bid_strike\", \"Ask_strike\"]]\n",
        "    #df[[\"Bid_last\", \"Ask_last\"]] = df[[\"Bid\", \"Ask\"]]\n",
        "    df[\"Price_last\"] = df[\"Price\"]\n",
        "    df = df.sort_values([\"Quote_date\"], ascending = [True])\n",
        "    return df\n",
        "\n",
        "def create_train_test(df, split_date):\n",
        "    \"\"\"Splits data in training and test set, and transforms data to right 2D format\"\"\"\n",
        "    return df[df[\"Quote_date\"] < split_date], df[df[\"Quote_date\"] >= split_date]\n",
        "\n",
        "def df_to_xy(df):\n",
        "    \"\"\"Transforms a dataframe into two arrays of explanatory variables x and explained variables y\"\"\"\n",
        "    dx = df[[\"Underlying_last\", \"Strike\", \"Ttl\", \"Volatility\", \"R\"]]\n",
        "    dy = df[\"Price\"]\n",
        "    array_x, array_y = dx.to_numpy().astype(np.float32), dy.to_numpy().astype(np.float32)\n",
        "    return array_x, array_y\n",
        "\n",
        "def min_max_scale(train, test):\n",
        "    \"\"\"Scales a training and test set using MinMaxScaler. The scaler is calibrated on the training set\"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    train = scaler.fit_transform(train)\n",
        "    test = scaler.transform(test)\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUf4mmpspOVv",
        "outputId": "62b87b7a-6054-4a19-acdb-4a94b7ad1e7b"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "df_read = read_file(\"../data/processed_data/2019-2021_underlying-strike_only-price.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEPU0KkgpQXH",
        "outputId": "7c661fbc-dc98-4008-e030-97997bb01bbe"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "num_models = 12\n",
        "\n",
        "features = [\"Underlying_last\", \"Strike\", \"Ttl\", \"Volatility\", \"R\"]\n",
        "seq_length = 5\n",
        "num_features = len(features)\n",
        "num_outputs = 1\n",
        "\n",
        "df_read_lags = lag_features(df_read, features, seq_length)\n",
        "\n",
        "train_val_test = []\n",
        "\n",
        "month = 4\n",
        "year = 0\n",
        "for i in range(num_models):\n",
        "    if month == 13:\n",
        "        year += 1\n",
        "        month = 1\n",
        "    train_start = datetime(2020 + year, month, 1)\n",
        "    val_start = train_start + relativedelta(months=8)\n",
        "    test_start = val_start + relativedelta(months=1)\n",
        "    test_end = test_start + relativedelta(months=1)\n",
        "\n",
        "    month += 1\n",
        "\n",
        "    df_train_orginal = df_read_lags.loc[(df_read_lags.loc[:, \"Quote_date\"] >= str(train_start)) & (df_read_lags.loc[:, \"Quote_date\"] < str(val_start)), :]\n",
        "    df_val_orginal = df_read_lags.loc[(df_read_lags.loc[:, \"Quote_date\"] >= str(val_start)) & (df_read_lags.loc[:, \"Quote_date\"] < str(test_start)), :]\n",
        "    df_test_orginal = df_read_lags.loc[(df_read_lags.loc[:, \"Quote_date\"] >= str(test_start)) & (df_read_lags.loc[:, \"Quote_date\"] < str(test_end)), :]\n",
        "\n",
        "    train_x_org, train_y_org = df_to_xy(df_train_orginal)\n",
        "    val_x_org, val_y_org = df_to_xy(df_val_orginal)\n",
        "    test_x_org, test_y_org = df_to_xy(df_test_orginal)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    train_x_scaled = scaler.fit_transform(train_x_org)\n",
        "    val_x_scaled = scaler.transform(val_x_org)\n",
        "    test_x_scaled = scaler.transform(test_x_org)\n",
        "\n",
        "    \"\"\"shuffle = np.random.permutation(len(train_x_scaled))\n",
        "    train_x_scaled, train_y_scaled = train_x_scaled[shuffle], train_y_scaled[shuffle]\"\"\"\n",
        "\n",
        "    train_x_scaled = np.reshape(train_x_scaled, (len(train_x_scaled), num_features))\n",
        "    val_x_scaled = np.reshape(val_x_scaled, (len(val_x_scaled), num_features))\n",
        "    test_x_scaled = np.reshape(test_x_scaled, (len(test_x_scaled), num_features))\n",
        "\n",
        "    # print(f\"Train_x shape: {train_x_scaled.shape}, train_y shape: {train_y_org.shape}\")\n",
        "    # print(f\"Test_x shape: {test_x_scaled.shape}, test_y shape: {test_y_org.shape}\")\n",
        "    # print(\"------------------------------------------------\")\n",
        "    train_val_test.append(((train_x_scaled, train_y_org), (val_x_scaled, val_y_org), (test_x_scaled, test_y_org)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "5ai3RgkWpVA3",
        "outputId": "a5f563c2-18c3-4626-c7b7-6fc747bc467e"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "import keras as KER\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.activations import linear, relu\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wtvth-7Vz87q"
      },
      "outputs": [],
      "source": [
        "def create_model(config):\n",
        "  \"\"\"Builds a model of minimum 2 layers sequentially from a given config dictionary\"\"\"\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(\n",
        "    units = config[\"units\"],\n",
        "    activation = relu,\n",
        "    input_shape = (config[\"num_features\"],)\n",
        "  ))\n",
        "\n",
        "  model.add(BatchNormalization(\n",
        "    momentum = config[\"bn_momentum\"]\n",
        "  ))\n",
        "\n",
        "\n",
        "  for i in range(config[\"layers\"]-2):\n",
        "    model.add(Dense(\n",
        "      units = config[\"units\"],\n",
        "      activation = relu\n",
        "    ))\n",
        "    model.add(BatchNormalization(\n",
        "      momentum = config[\"bn_momentum\"]\n",
        "    ))\n",
        "\n",
        "  model.add(Dense(\n",
        "    units = config[\"units\"],\n",
        "    activation = relu\n",
        "  ))\n",
        "\n",
        "  model.add(BatchNormalization(\n",
        "    momentum = config[\"bn_momentum\"]\n",
        "  ))\n",
        "\n",
        "  model.add(Dense(\n",
        "    units = num_outputs,\n",
        "    activation = relu\n",
        "  ))  \n",
        "\n",
        "  model.compile(\n",
        "    optimizer = AdamW(\n",
        "      learning_rate = config[\"learning_rate\"],\n",
        "      weight_decay = config[\"weight_decay\"]\n",
        "    ),\n",
        "    loss = \"mse\",\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMFVcYRHpWNo",
        "outputId": "158d9438-08de-4d33-9f64-3bc2f2460ee3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_48          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_49          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_50          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_51          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_60 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m192\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_48          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_61 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_49          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_62 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_50          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_63 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_51          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_64 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,905</span> (15.25 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,905\u001b[0m (15.25 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,649</span> (14.25 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,649\u001b[0m (14.25 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1068984.0000 - val_loss: 731658.2500\n",
            "Epoch 2/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 574335.0000 - val_loss: 155501.0938\n",
            "Epoch 3/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 107305.3438 - val_loss: 9192.6895\n",
            "Epoch 4/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4849.9995 - val_loss: 289.0832\n",
            "Epoch 5/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 576.2460 - val_loss: 996.8809\n",
            "Epoch 6/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 542.3729 - val_loss: 143.3677\n",
            "Epoch 7/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 515.4399 - val_loss: 283.8382\n",
            "Epoch 8/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 518.2140 - val_loss: 170.0995\n",
            "Epoch 9/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 502.9921 - val_loss: 163.7870\n",
            "Epoch 10/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 497.9493 - val_loss: 161.6674\n",
            "Epoch 11/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 493.0106 - val_loss: 158.9072\n",
            "Epoch 12/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 489.0452 - val_loss: 150.3173\n",
            "Epoch 13/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 485.9208 - val_loss: 142.4375\n",
            "Epoch 14/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 483.5494 - val_loss: 141.7563\n",
            "Epoch 15/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 481.9435 - val_loss: 138.8594\n",
            "Epoch 16/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 480.7494 - val_loss: 135.5257\n",
            "Epoch 17/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 479.4058 - val_loss: 128.5562\n",
            "Epoch 18/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 478.1820 - val_loss: 122.0656\n",
            "Epoch 19/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 479.6733 - val_loss: 155.0331\n",
            "Epoch 20/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 483.5138 - val_loss: 123.3723\n",
            "Epoch 21/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 479.5191 - val_loss: 117.5603\n",
            "Epoch 22/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 478.3545 - val_loss: 120.1949\n",
            "Epoch 23/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 477.5201 - val_loss: 120.5492\n",
            "Epoch 24/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 476.5368 - val_loss: 110.6970\n",
            "Epoch 25/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 475.5265 - val_loss: 107.3870\n",
            "Epoch 26/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 474.9083 - val_loss: 104.9098\n",
            "Epoch 27/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 474.4080 - val_loss: 103.1567\n",
            "Epoch 28/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 473.7240 - val_loss: 99.0998\n",
            "Epoch 29/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 473.0525 - val_loss: 99.7800\n",
            "Epoch 30/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 472.4211 - val_loss: 95.5380\n",
            "Epoch 31/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 471.7576 - val_loss: 94.1983\n",
            "Epoch 32/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 470.8461 - val_loss: 93.3720\n",
            "Epoch 33/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 470.1974 - val_loss: 101.4585\n",
            "Epoch 34/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 469.8382 - val_loss: 119.1338\n",
            "Epoch 35/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 468.7065 - val_loss: 116.2383\n",
            "Epoch 36/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 468.0505 - val_loss: 119.2886\n",
            "Epoch 37/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 467.2851 - val_loss: 119.5096\n",
            "Epoch 38/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 466.6942 - val_loss: 119.4718\n",
            "Epoch 39/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 466.1901 - val_loss: 122.6162\n",
            "Epoch 40/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 465.7380 - val_loss: 120.6170\n",
            "Epoch 41/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 465.6066 - val_loss: 136.6467\n",
            "Epoch 42/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 465.5370 - val_loss: 127.7569\n",
            "Epoch 43/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 464.5233 - val_loss: 127.5013\n",
            "Epoch 44/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 464.2655 - val_loss: 201.5352\n",
            "Epoch 45/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 463.9445 - val_loss: 129.9135\n",
            "Epoch 46/100\n",
            "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 463.4231 - val_loss: 123.2602\n",
            "Test loss: 498.7407\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "\n",
        "config = {\n",
        "    \"units\": 32,\n",
        "    \"learning_rate\": 0.004469423596275494,\n",
        "    \"layers\": 4,\n",
        "    \"seq_length\": seq_length,\n",
        "    \"num_features\": num_features,\n",
        "    \"bn_momentum\" : 0.30057069329591907,\n",
        "    \"weight_decay\" : 0.00042470893538329376,\n",
        "}\n",
        "\n",
        "def trainer(train_x, train_y, model, val_x, val_y):\n",
        "    epochs = 100\n",
        "    minibatch_size = 4096\n",
        "\n",
        "    tf.random.set_seed(2)\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        min_delta = 1,\n",
        "        patience = 15,\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        train_x,\n",
        "        train_y,\n",
        "        batch_size = minibatch_size,\n",
        "        validation_data = (val_x, val_y),\n",
        "        epochs = epochs,\n",
        "        callbacks = [early_stopping]\n",
        "    )\n",
        "\n",
        "predictions = []\n",
        "for i, ((x_train, y_train), (x_val, y_val), (x_test, y_test)) in enumerate(train_val_test):\n",
        "    if i == 10:\n",
        "        model = create_model(config)\n",
        "        model.summary()\n",
        "        trainer(x_train, y_train, model, x_val, y_val)\n",
        "\n",
        "        pred = np.array(model(x_test)).flatten()\n",
        "        print(\"Test loss:\", np.mean((pred-y_test)**2))\n",
        "\n",
        "        predictions.append(np.array(model(x_test)))\n",
        "\n",
        "predictions = np.concatenate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17p20nE8yLxu",
        "outputId": "13a90785-3b58-44e8-c054-762cb82a60ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3t/5vvh8l5x48s7tyvbx4b6v9xr0000gn/T/ipykernel_11883/2103501810.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test[\"Prediction\"] = predictions\n"
          ]
        }
      ],
      "source": [
        "def prediction(df_test, predictions):\n",
        "    # df_test[\"Prediction\"] = predictions.flatten()\n",
        "    df_test[\"Prediction\"] = predictions\n",
        "    return df_test\n",
        "\n",
        "df_test_whole = df_read_lags.loc[df_read_lags.loc[:, \"Quote_date\"] >= \"2021-01-01\", :]\n",
        "df_test_whole = prediction(df_test_whole, predictions)\n",
        "\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "time = datetime.now()\n",
        "time = time.strftime(\"%m-%d_%H-%M\")\n",
        "\n",
        "filename = f\"../data/Predictions/{last_year}_predictions_{time}.csv\"\n",
        "filepath = Path(filename)\n",
        "filepath.parent.mkdir(parents=True, exist_ok = True)\n",
        "df_test_whole.to_csv(filename)\n",
        "#df_test.info()\n",
        "#print(df_test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1845482 entries, 0 to 1845481\n",
            "Data columns (total 38 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Unnamed: 0.1          int64  \n",
            " 1   Unnamed: 0            int64  \n",
            " 2   Quote_date            object \n",
            " 3   Expire_date           object \n",
            " 4   Price                 float64\n",
            " 5   Underlying_last       float64\n",
            " 6   Strike                float64\n",
            " 7   Ttl                   int64  \n",
            " 8   Volatility            float64\n",
            " 9   Volatility_GJR_GARCH  float64\n",
            " 10  R                     float64\n",
            " 11  Underlying_last-4     float64\n",
            " 12  Strike-4              float64\n",
            " 13  Ttl-4                 float64\n",
            " 14  Volatility-4          float64\n",
            " 15  R-4                   float64\n",
            " 16  Underlying_last-3     float64\n",
            " 17  Strike-3              float64\n",
            " 18  Ttl-3                 float64\n",
            " 19  Volatility-3          float64\n",
            " 20  R-3                   float64\n",
            " 21  Underlying_last-2     float64\n",
            " 22  Strike-2              float64\n",
            " 23  Ttl-2                 float64\n",
            " 24  Volatility-2          float64\n",
            " 25  R-2                   float64\n",
            " 26  Underlying_last-1     float64\n",
            " 27  Strike-1              float64\n",
            " 28  Ttl-1                 float64\n",
            " 29  Volatility-1          float64\n",
            " 30  R-1                   float64\n",
            " 31  Underlying_last-0     float64\n",
            " 32  Strike-0              float64\n",
            " 33  Ttl-0                 int64  \n",
            " 34  Volatility-0          float64\n",
            " 35  R-0                   float64\n",
            " 36  Price_last            float64\n",
            " 37  Prediction            float64\n",
            "dtypes: float64(32), int64(4), object(2)\n",
            "memory usage: 535.0+ MB\n",
            "   Unnamed: 0.1  Unnamed: 0  Quote_date Expire_date     Price  \\\n",
            "0       3102998     4482920  2021-01-04  2021-05-28   919.600   \n",
            "1       3103091     4483013  2021-01-04  2021-05-28    69.045   \n",
            "2       3104380     4484302  2021-01-04  2022-03-18  1310.100   \n",
            "3       3104484     4484406  2021-01-04  2022-06-17  2143.910   \n",
            "4       3097922     4477844  2021-01-04  2021-01-06    84.595   \n",
            "\n",
            "   Underlying_last  Strike  Ttl  Volatility  Volatility_GJR_GARCH  ...  Ttl-1  \\\n",
            "0          3701.38  2800.0  144    0.185353              0.088931  ...  148.0   \n",
            "1          3701.38  3950.0  144    0.185353              0.088931  ...  148.0   \n",
            "2          3701.38  2400.0  438    0.185353              0.088931  ...  442.0   \n",
            "3          3701.38  1500.0  529    0.185353              0.088931  ...  533.0   \n",
            "4          3701.38  3625.0    2    0.185353              0.088931  ...    6.0   \n",
            "\n",
            "   Volatility-1   R-1  Underlying_last-0  Strike-0  Ttl-0  Volatility-0   R-0  \\\n",
            "0      0.183485  0.09            3701.38    2800.0    144      0.185353  0.09   \n",
            "1      0.183485  0.09            3701.38    3950.0    144      0.185353  0.09   \n",
            "2      0.183485  0.10            3701.38    2400.0    438      0.185353  0.10   \n",
            "3      0.183485  0.10            3701.38    1500.0    529      0.185353  0.10   \n",
            "4      0.183485  0.08            3701.38    3625.0      2      0.185353  0.09   \n",
            "\n",
            "   Price_last   Prediction  \n",
            "0     919.600   916.550660  \n",
            "1      69.045    56.947580  \n",
            "2    1310.100  1320.769900  \n",
            "3    2143.910  2176.435000  \n",
            "4      84.595    71.554924  \n",
            "\n",
            "[5 rows x 38 columns]\n"
          ]
        }
      ],
      "source": [
        "# Run this if one month is very bad\n",
        "\n",
        "if False:\n",
        "    df_test_whole = pd.read_csv(\"../data/Predictions/2021_predictions_09-30_09-45.csv\")\n",
        "    df_test_whole.loc[(df_test_whole.loc[:, \"Quote_date\"] >= \"2021-11-01 00:00:00\") & (df_test_whole.loc[:, \"Quote_date\"] < \"2021-12-01 00:00:00\"), \"Prediction\"] = predictions\n",
        "\n",
        "    from datetime import datetime\n",
        "    time = datetime.now()\n",
        "    time = time.strftime(\"%m-%d_%H-%M\")\n",
        "\n",
        "    filename = f\"../data/Predictions/{last_year}_predictions_{time}.csv\"\n",
        "    filepath = Path(filename)\n",
        "    filepath.parent.mkdir(parents=True, exist_ok = True)\n",
        "    df_test_whole.to_csv(filename)\n",
        "\n",
        "    df_test_whole.info()\n",
        "    print(df_test_whole.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.1 ('A_Star-bay7YQ3K')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "346f1978fc3949933b17c75d4815baa97117852380bde1b98c856e676f6b5766"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
